{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUWZ3tuc51HC"
   },
   "source": [
    "# Data Augmentation for Aspect-Conditioned Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSfC6lMB51HD"
   },
   "source": [
    "The goal is to mechanically and automatically download reports and analyse them using different powerful and sophisticated Deep Learning architecture for aspect-based sentiment analysis, and to evaluate it after adding more data points for the aspect embeddings to the training set. Therefore you are supposed to perform either random sampling around word vectors and/or choosing synonyms using synonym dictionary libraries.\n",
    "\n",
    "References (Libraries)\n",
    "https://github.com/rwik/python-mythes\n",
    "https://github.com/sbos/AdaGram.jl/blob/master/README.md\n",
    "https://gitlab.lrz.de/dugarsumit/simsent (needs privileges)\n",
    "\n",
    "References (Literature)\n",
    "Master Thesis of Sumit Dugar: Aspect-Based Sentiment Analysis Using Deep Neural Networks and Transfer Learning\n",
    "\n",
    "### Group 12: Rajdeep Surolia, Tuhin Ghosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esBVR8rJ51HE"
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScbOOOu651HF"
   },
   "source": [
    " ### 1 Introduction \n",
    " ### 2 Background \n",
    " ### 3 Establishing Baselines\n",
    " ### 4 The Deep-Net Architecture\n",
    " ### 5 Experiment and Result\n",
    " ### 6 Conclusion\n",
    " ### 7 Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qv-ca8sh51HF"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWiQCSMq51HG"
   },
   "source": [
    "### 1.1 Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHICnFI251HH"
   },
   "source": [
    "Aspect-based Sentiment Analysis (ABSA) is the task of detecting aspects and sentiments expressed towards those aspects from a text given in a natural language. Some basic examples are shown in table 1.1. Traditionally, sentiment has been defined using a discrete polarity scale of positive, neutral, or negative opinion towards something (Mäntylä et al., 2016). However, depending on the requirements it can also be defined on a continuous scale ranging from highly negative to highly positive. Input text can either be in the form of a document representing a review, a comment, or a tweet, or it can be a single sentence. The situation with the definition of an aspect is not much different. Some define it as a single category label e.g - price, cleanliness. Others define it as a tuple of an entity and an attribute, for instance, <food, quality>, <graphics, general>.\n",
    "\n",
    "In this work, we will adhere to the latter definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m16_ClT51HH"
   },
   "source": [
    "![Screen%20Shot%202019-07-28%20at%2010.12.04%20PM.png](attachment:Screen%20Shot%202019-07-28%20at%2010.12.04%20PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fx8QGTlu51HI"
   },
   "source": [
    "<font color=blue> Table 1: Some basic examples of aspect based sentiment analysis. First two sentences are taken from SemEval-2016 dataset of restaurant and laptop domain respectively Pontiki et al. (2016a). Last sentence is from our self annotated organic food dataset. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_cygkGp51HJ"
   },
   "source": [
    "As per Pontiki et al. (2016b), Lakkaraju et al. (2014), Schouten and Frasincar (2016) and many others, extraction of aspects is not considered a primary task for ABSA. According to them, aspect extraction is a separate task which is completed prior to ABSA and hence the primary goal of ABSA is to find the sentiment polarity given the extracted aspect and the text. Wojatzki et al. (2017b), Schmitt et al. (2018) have defined ABSA slightly differently by taking it one step ahead. In their setting, both detection of aspects (out of some finite pre-decided aspects) and sentiments (expressed towards those aspects) are considered primary tasks of ABSA. In this study of work, we will be working with the second formalism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtFh1CX951HJ"
   },
   "source": [
    "### 1.2 Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHygf0RF51HK"
   },
   "source": [
    "Sentiment Analysis or ABSA is not an easy task to solve. We are broadly dealing with 4 different problems here. \n",
    "<br>\n",
    " - The subtle ways in which humans use natural language. To better understand this point consider the following example sentence taken from Pang and Lee (2008).<br>\n",
    "<font color=green> *“If you are reading this because it is your darling fragrance, please wear it at home exclusively, and tape the windows shut.” </font> - (a review by Luca Turin and Tania Sanchez of the Givenchy perfume Amarige, in Perfumes: The Guide, Viking 2008.)*\n",
    "\n",
    "In this sentence, the author has expressed a highly negative sentiment apparently without using any negative words or phrases. So, situations like these make sentiment analysis and ABSA a difficult problem to solve. \n",
    " - Additionally, ABSA should not only identify the sentiment but it should also detect the aspect (towards which sentiment was expressed). Generally, expressions and words used to convey sentiments are common across different domains. Words such as - dazzling, brilliant, phenomenal convey positive sentiments and words such as - suck, terrible, awful convey negative sentiments no matter in which domain they are used. But there could be exceptional situations where the same phrase may express different sentiments for different domains. For instance, consider this example from Pang and Lee (2008).\n",
    " <font color=green> *“Go read the book.”* </font> <br>\n",
    "This is likely a positive sentiment for books but probably a negative one for movies. So if the algorithm knows in advance that the current review is talking about the movie domain then, it could make use of this additional information to predict sentiment more accurately. So, how important is the contextual knowledge for correctly identifying sentiments expressed in a sentence? From the last example, it is clear that having some form of contextual knowledge about the subject being reviewed can definitely help in predicting sentiments more accurately. __Hence, how to effectively incorporate contextual information is one of the problems that this work is trying to tackle.__\n",
    "<br>\n",
    " - It is not very scalable to train and maintain ABSA models separately for individual domains. Also, not every domain has sufficient labeled data. __Therefore, discovering techniques to effectively train an ABSA model with a limited amount of data Or somehow transfer the knowledge of an already trained model (on a source domain) to another model (on a target domain) are some other directions that this study of work in looking into.__\n",
    "<br>\n",
    " - The availability of raw data is much more than it was ever before, yet it is difficult to find labeled data. This is an important blockade not just for NLP but also for other fields that are exploring deep learning methods. Fields like computer vision that mostly rely on image data have developed many augmentation techniques for artificially generating additional data. For textual data, augmentations are not that intuitive and hence only a handful of techniques exists. __This work also tries to study the effects of one of these augmentation techniques on ABSA.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSfRHldE51HL"
   },
   "source": [
    "### 1.3 Proposed Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVFkHD-851HL"
   },
   "source": [
    "- This work is a step towards finding answers to the aforementioned problems. The main contribution of this project is a novel deep learning architecture that is capable of jointly detecting aspects and their corresponding sentiments. The architecture is inspired by Schmitt et al. (2018), Wang et al. (2016), Ruder et al. (2016a), Yang et al. (2016a). Another important contribution of this work is a technique for the transformation/pre- processing of input data. This technique provides some interesting benefits to the proposed model. It helps in addressing some of the problems mentioned above, such as - a huge number of output classes, limitation of labeled data, transfer of knowledge from one domain to another. \n",
    "- In order to be context-aware, the network takes the entire comment/review as input. Now, while making predictions for each sentence the network uses contextual knowledge derived from the entire review. The network is implemented using RNN and attention layer which helps in learning the contextual information. \n",
    "- To address the concern about the limitation of training data and its imbalance, a data augmentation technique was explored. In this technique, similar sentences were generated by making use of synonyms or similar words. Additionally, the input transformation also provided a kind of data augmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruvby_Bo51HM"
   },
   "source": [
    "## 2. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYg4x_BN51HN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFiE3yM451HP"
   },
   "source": [
    "## 3. Establishing Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTKPE6G951HQ"
   },
   "source": [
    " - We trained on the SemEval Dataset using 3 classifiers - 1. Support Vector Classifier(SVC), 2. Linear SVC and 3. SGDClassifier using the skLearn library. This is based on multi-class classification. \n",
    " - We fed 2 different types of word embeddings. One was a naive approach using sklearn's CountVectorizer and the other much advanced one, facebook's fastText to get a better result.\n",
    " - It was a 2 step pipeline. First, we predicted reviews having opinions and then, we predicted the aspects of those reviews. \n",
    " - For this, we took the 20 most common aspects among all the aspects, since some of the dataset had a huge number of aspect categories (SemEval 2014). \n",
    " - Then, we proceeded with the usual cleaning and processing of the data via tokenizing, vectorizing, converting into dataframes and finally geting the respective word embeddings.\n",
    " - Finally, we trained them on the 3 classifiers and evaluated the F1-scores.\n",
    " - The results were suprising to say the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuIT3wj0TT-U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gIHiNmU51HR"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%2012.11.51%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%2012.11.51%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYhQlKFV51HR"
   },
   "source": [
    "<font color=blue> Table 2: Comparison between the micro F1 scores when using CountVectorizer and when using fastText </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MmV8xfQ51HS"
   },
   "source": [
    " - As you can see from the above table, fastText performed much worse than the CountVectorizer. This is very puzzling because the fastText was used on the sole purpose to get a better prediction\n",
    " - We further got results for each of the 4 classes individually.\n",
    " - One last thing that we did was to perform the whole pipeline on an example review so as to get full picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHqS5QwN51HT"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%2012.20.45%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%2012.20.45%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tumvR1cy51HU"
   },
   "source": [
    "<font color=blue> Figure 1: Represents the 20 most common aspects. Prediction of whether or not the review does have an aspect belonging to a particular aspect. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePnTVYCJ51HU"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%2012.32.06%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%2012.32.06%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eArO_bs851HV"
   },
   "source": [
    "<font color=blue> Figure 2: Predicting the respective opinion of the aspects belonging to the review </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqrQ_W4551HV"
   },
   "source": [
    "## 4. The Deep-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQJnu59151HW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddogrunD51Ha"
   },
   "source": [
    "## 5. Experiment and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sHhHSZ651Hb"
   },
   "source": [
    "### 5.1 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-LJ7y5051Hb"
   },
   "source": [
    " - For augmenting the data we use an approach where we replace a word either with its synonym or with a similar word. As the augmentations generated using the synonym approach looked rather poor so, we decided to adhere to the similar word approach. \n",
    " - An important thing to note is that for generating similar words we have used GloVe embeddings because with fastText embeddings a lot of the times generated similar words were just the n-grams of the original word. \n",
    " - Similar words don’t necessarily imply words with similar meaning. Two words are said to be similar if they are very close in the embedding space. Sometimes it might happen that words which are similar according to the embedding space can be opposite in meaning. Because of this, we have used this approach to only augment neutral sentences.\n",
    " - In the SemEval 2016 dataset for exaple, the neutral class is highly imbalanced consisting of only 0.41% of the total data. After applying the augmentation it got improved a bit to about 6.04%. \n",
    " - We got these results by limiting the number of similar words for a given word to three. That is, for every replaceable word in a sentence we generated a maximum of three similar words and used these for generating the paraphrases. \n",
    " - Using more words led to the generation of a lot more paraphrases which in turn made the data imbalanced with respect to positive and negative classes so, we just used the top three similar words for generating augmented sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRMyyPc-51Hc"
   },
   "source": [
    "### 5.2 Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmmezEqv51Hd"
   },
   "source": [
    "Majority of the implementation was done by us while some small modules were borrowed from a repository on Hierarchical Attention Models by Ezhov (2017). The architecture was implemented using TensorFlow 1.8.0. Details of the training environment are :\n",
    "\n",
    "1. Google Colab\n",
    "2. Local System (12GB GeForce GTX TITAN X, Ubuntu 18.04.2, Python 3.7.0, RAM 64GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9flvtO8y51Hd"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%2012.41.37%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%2012.41.37%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImzpBA_r51He"
   },
   "source": [
    "<font color=blue> Table 3: This table shows a list of hyperparameters values, these were fixed either by using a grid search or trail & error or were directly picked from the literature. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9oFjs5o51Hf"
   },
   "source": [
    "We used 2 different types of embeddings for this training, fastText and GloVe. We can observe some improvement in the overall performance after training the model with the augmented dataset. The performance gain is significant in case of fastText embedding when compared to a model that was trained without augmentation (PT-F) than to a model that was trained with augmentation (PT-F + DA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WsXIiMx51Hf"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%201.01.19%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%201.01.19%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iEpZZhN51Hg"
   },
   "source": [
    "<font color=blue> Figure 3: This table shows the performance of different variants of our model on SemEval 2016 Restaurant Dataset. Pre-trained version used GloVe/fastText embeddings. For data augmentation we generated similar sentences using similar words approach. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVOv06oB51Hg"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%201.21.40%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%201.21.40%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5UzzuQm51Hh"
   },
   "source": [
    "<font color=blue> Figure 4: This table shows the performance of our model on SemEval-2016 Laptops dataset for ABSA. We have used pre-trained GloVe/fastText embeddings. For data augmenta- tion we generated similar sentences using similar words. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTurah_F51Hi"
   },
   "source": [
    "![Screen%20Shot%202019-07-29%20at%201.05.28%20AM.png](attachment:Screen%20Shot%202019-07-29%20at%201.05.28%20AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igkJ_SQn51Hi"
   },
   "source": [
    "<font color=blue> Figure 5: This table shows the performance of different variants of our model for the aspect detection task. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ls9uKMja51Hj"
   },
   "source": [
    "### 5.3 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftmvAZ4451Hj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final_Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
